{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1\n",
    ")\n",
    "# temperature: 얼마나 창의적인가 0~1\n",
    "\n",
    "template = PromptTemplate.from_template(\n",
    "    \"{country_a}과(와) {country_b} 사이의 거리는 어떻게 되나요?\"\n",
    ")\n",
    "\n",
    "prompt = template.format(country_a = \"한국\", country_b= \"일본\")\n",
    "\n",
    "\n",
    "chat.predict(prompt)\n",
    "\n",
    "# from langchain.llms import OpenAI\n",
    "# llm = OpenAI(model_name=\"gpt-3.5-turbo-0125\")\n",
    "# a = llm.predict(\"How many planets are there?\")\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography export. And you only reply in {language}.\"),\n",
    "    (\"ai\", \"안녕하세요. 제 이름은 {name}입니다.\"),\n",
    "    (\"human\", \"{country_a}과(와) {country_b} 사이의 거리는 어떻게 되나요?. 그리고 당신의 이름은 무엇인가요?\"),\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    language = \"korean\",\n",
    "    name = \"쫀떡\",\n",
    "    country_a = \"한국\",\n",
    "    country_b = \"일본\",\n",
    ")\n",
    "\n",
    "chat.predict_messages(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "    # strip: text 앞뒤 공백을 잘라준다. -> 텍스트ㅡ이 시작부분이나 끝부분이 공백이라면 제거\n",
    "    \n",
    "p = CommaOutputParser()\n",
    "p.parse(\"Hello,   how,are,you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 list 생성 기계입니다. 질문들은 모두 최대 {max_items}개, comma로 구분해주세요. 그리고 영어로 답할 경우 소문자로만 답해주세요. list가 아닌 것으로 답을 하지마세요\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "    ])\n",
    "\n",
    "prompt = template.format_messages(max_items=  \"10\", question = \"What are the colors?\")\n",
    "result = chat.predict_messages(prompt)\n",
    "p = CommaOutputParser()\n",
    "p.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "    # strip: text 앞뒤 공백을 잘라준다. -> 텍스트ㅡ이 시작부분이나 끝부분이 공백이라면 제거\n",
    "    \n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 list 생성 기계입니다. 질문들은 모두 최대 {max_items}개, comma로 구분해주세요. 그리고 영어로 답할 경우 소문자로만 답해주세요. list가 아닌 것으로 답을 하지마세요\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "    ])\n",
    "\n",
    "prompt = template.format_messages(max_items=  \"10\", question = \"What are the colors?\")\n",
    "result = chat.predict_messages(prompt)\n",
    "p = CommaOutputParser()\n",
    "p.parse(result.content)\n",
    "\n",
    "# chat model, outputParser, template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain: 모든 요소를 합쳐주는 역할을 한다.\n",
    "# 합쳐진 요소들은 하나의 chain으로 실행된다. \n",
    "\n",
    "from langchain.chat_models import ChatOpenAI, ChatAnthropic\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "# ChatModel\n",
    "# chat = ChatAnthropic(temperature=0.1)\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "\n",
    "# OutputParser\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "    # strip: text 앞뒤 공백을 잘라준다. -> 텍스트ㅡ이 시작부분이나 끝부분이 공백이라면 제거\n",
    "    \n",
    "    \n",
    "# Prompt\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 list 생성 기계입니다. 질문들은 모두 최대 {max_items}개, comma로 구분해주세요. 그리고 영어로 답할 경우 소문자로만 답해주세요. list가 아닌 것으로 답을 하지마세요\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "    ])\n",
    "\n",
    "\n",
    "chain_one = template | chat | CommaOutputParser()\n",
    "# chain_two = template_2 | chat | outputparser\n",
    "# all = chain_one | chain_two | outputparser \n",
    "\n",
    "chain_one.invoke({\n",
    "    \"max_items\": 5,\n",
    "    \"question\": \"What are the pokemons?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "# streaming: model의 응답이 생성되는 걸 볼 수 있게 해준다.\n",
    "\n",
    "chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 세계적인 요리사입니다. 당신은 쉬운 제료로 쉽게 만들 수 있는 음식 제조법을 알려줘야 합니다.\"),\n",
    "    (\"human\", \"나는 {cuisine}을 만들고 싶습니다.\"),\n",
    "])\n",
    "\n",
    "chef_chain = chef_prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a vegetarian chef specialized on making traditional recipes vegetarian. You find alternative ingredients and explain their preparation. You don't radically modify the recipe. If there is no alternative for a food just say you don't know how to recipe it.\"),\n",
    "    (\"human\", \"{recipe}\"),\n",
    "])\n",
    "\n",
    "veg_chef_chain = veg_chef_prompt | chat_model\n",
    "\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chef_chain # veg_chain의 입력값 recipe를 chef_chain으로부터 뱓는다.\n",
    "final_chain.invoke({\n",
    "    \"cuisine\":\"카레\",\n",
    "})\n",
    "# chef_chain으로부터 얻은 결과가 recipe라는 key에 저장된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모듈\n",
    "LangChain은 다음과 같은 주요 모듈을 위한 확장 가능한 표준 인터페이스 및 외부 통합 기능을 제공합니다:\n",
    "\n",
    "- Model I/O\n",
    "언어 모델과의 인터페이스\n",
    "\n",
    "- Retrieval\n",
    "애플리케이션별 데이터를 사용한 인터페이스\n",
    "\n",
    "- Agents\n",
    "높은 수준의 지침이 주어지면 체인이 어떤 도구를 사용할지 선택하도록 합니다\n",
    "\n",
    "- Chains\n",
    "일반적인 빌딩 블록 구성\n",
    "\n",
    "- Memory\n",
    "체인 실행 간 지속적인 응용 프로그램 상태\n",
    "\n",
    "- Callbacks\n",
    "모든 체인의 중간 단계 기록 및 스트리밍\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# t = PromptTemplate(\n",
    "#     template=\"What is the capital of {country}\",\n",
    "#     input_variables = [\"country\"],\n",
    "# )\n",
    "t = PromptTemplate.from_template(\"What is the capital of {country}\")\n",
    "t.format(country=\"korea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "    Human: {question}\n",
    "    Ai: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# example_prompt = PromptTemplate.from_template(example_template)\n",
    "# example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAi: {answer}\")\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{country}에 대해 알려줘\"),\n",
    "    (\"ai\", \"{answer}\")\n",
    "])\n",
    "\n",
    "# prompt = FewShotPromptTemplate(\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    # suffix=\"Human: What do you know about {country}?\",\n",
    "    # input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 지리 전문가야\"),\n",
    "    example_prompt,\n",
    "    (\"human\", \"{country}에 대해 알려줘\" ),\n",
    "])\n",
    "\n",
    "chain = final_prompt | chat_model\n",
    "chain.invoke({\n",
    "    \"country\": \"Korea\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "    Human: {question}\n",
    "    Ai: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{country}에 대해 알려줘\"),\n",
    "    (\"ai\", \"{answer}\")\n",
    "])\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 지리 전문가야\"),\n",
    "    example_prompt,\n",
    "    (\"human\", \"{country}에 대해 알려줘\" ),\n",
    "])\n",
    "\n",
    "chain = final_prompt | chat_model\n",
    "chain.invoke({\n",
    "    \"country\": \"Korea\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# 예제 선택기 \n",
    "class RandomExampleSelect(BaseExampleSelector):\n",
    "    \n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "    \n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "    \n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "        return [choice(self.examples)]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAi: {answer}\")\n",
    "\n",
    "example_selector = RandomExampleSelect(\n",
    "    examples = examples,\n",
    ")\n",
    "\n",
    "example_prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix = \"Human: {country}에 대해 알려줘\",\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Korea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialization\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "# prompt = load_prompt(\"./prompt.json\")\n",
    "prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "prompt.format(country= \"Korea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    당신은 역할놀이 보조자입니다. 그리고 당신은 {character} 역을 맡아야 합니다.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    이건 어떻게 말해야 하는지 예시야:\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "    시작!\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "    {intro}\n",
    "    \n",
    "    {example}\n",
    "    \n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start)\n",
    "]\n",
    "full_prompt = PipelinePromptTemplate(final_prompt=final, pipeline_prompts=prompts)\n",
    "\n",
    "# 확인\n",
    "# full_prompt.format(\n",
    "#     character = \"해적\",\n",
    "#     example_question = \"어디에 있어?\",\n",
    "#     example_answer = \"크하하하! 그건 비밀이다!\",\n",
    "#     question = \"너가 가장 좋아하는 음식은 뭐야?\"\n",
    "# )\n",
    "\n",
    "chain = full_prompt | chat_model\n",
    "chain.invoke({\n",
    "    \"character\": \"해적\",\n",
    "    \"example_question\": \"어디에 있어?\",\n",
    "    \"example_answer\": \"크하하하! 그건 비밀이다!\",\n",
    "    \"question\": \"너가 가장 좋아하는 음식은 뭐야?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caching: 같은 질문을 계속 받으면 답변을 재사용해 비용을 절약할 수 있게 해준다.\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "# set_llm_cache(InMemoryCache())\n",
    "set_debug(True)\n",
    "# 무슨 일을 하고 있는지에 대한 로그를 보여준다.\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "chat_model.predict(\"토마토 스파게티는 어떻게 만들어?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model.predict(\"토마토 스파게티는 어떻게 만들어?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지출하는 비용을 아는 방법\n",
    "# 모델을 어떻게 저장하고 불러오는지에 대한 방법\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "with get_openai_callback() as test:\n",
    "    a = chat_model.predict(\"소주 만드는 방법 알려줘\")\n",
    "    b = chat_model.predict(\"토마토 스파게티 만드는 법 알려줘\")\n",
    "    print(a, b, \"\\n\")\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms.openai import OpenAI\n",
    "\n",
    "chat = OpenAI(temperature=0.1, max_tokens=450, model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "chat.save(\"model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "chat = load_llm(\"model.json\")\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain에는 여러 종류의 메모리가 있다. / 각자 저장 방식도 다르다\n",
    "# 챗봇에 메모리를 추가하지 않으면 기억할 수 없다.\n",
    "# 메모리 종류, 차이점 -> 메모리 탑재 방법\n",
    "\n",
    "\n",
    "# 1. Conversation Buffer = 단순, 이전 대화 내용 전체를 저장 / 대화가 길어지면 메모리도 커져 비효율적이다.\n",
    "# text completion 할 때 유용, 예측할 때, 텍스트를 자동완성하고 싶을 때 \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# memory = ConversationBufferMemory()\n",
    "memory = ConversationBufferMemory(return_messages=True) # 챗모델 사용\n",
    "\n",
    "memory.save_context({\"input\":\"Hi\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. conversation Buffer Window Memory\n",
    "# 대화 기록 최소화, 가장 오래된 메시지는 버린다. -> 모든 대화 내용을 저장하지 않아도 된다. / 최근 대화에만 집중한다.\n",
    "# 메모리의 사이즈가 게속 늘어나지 않지만, 챗봇이 예전 대화를 기억하지 못 한다.\n",
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k = 4 # 버퍼 윈도우의 사이즈 = 몇 개의 메세지를 저장할 것인가\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(1, 1)\n",
    "add_message(2, 2)\n",
    "add_message(3, 3)\n",
    "add_message(4, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Conversation Summary Memory\n",
    "# message를 그대로 저장하지 않고 요약을 자체적으로 해준다. -> 긴 conversation에서 효과적이다.\n",
    "# 초반에는 Conversation Summary Memory는 이전보다 더 많은 토큰과 저장공간을 차지하지만 이후에 효과적\n",
    "\n",
    "# 시간이 지남에 따라 대화의 요약 -> 시간이 지남에 따라 대화의 정보를 압축하는 데 유용\n",
    "# 대화가 진행되는 대로 요약하고 현재 요약 내용을 메모리에 저장 \n",
    "\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "    \n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"안녕 나는 장태훈이야, 나는 한국에서 살아\", \"와 이거 정말 멋진데!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"한국은 정말 멋져!!\", \"나도 갈 수 있었으면 좋겠다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human introduces themselves as Jang Taehoon from Korea. The AI responds enthusiastically to the introduction, expressing a desire to visit Korea as well.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
