{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model I/O\n",
    "> LangChain은 주요 모듈을 위한 확장 가능한 표준 인터페이스 및 외부 통합 기능을 제공\n",
    "> - model I/O: 언어 모델과의 인터페이스\n",
    "\n",
    "> - Retrieval: 애플리케이션별 데이터를 사용한 인터페이스\n",
    "> - - Retrieval: 외부 데이터로 접근해 이를 모델에 어떻게 제공하는 것에 관한 것\n",
    "\n",
    "> - chain: 일반적인 빌딩 블록 구성\n",
    "\n",
    "> - memory: 체인 실행 간 지속적인 응용 프로그램 상태\n",
    "\n",
    "> - callback: 모든 체인의 중간 단계 기록 및 스트리밍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# t = PromptTemplate.from_template(\"{country}의 수도는 어디야?\")\n",
    "t = PromptTemplate(\n",
    "    template = \"{country}의 수도는 어디야?\",\n",
    "    input_variables = [\"country\"],\n",
    ")\n",
    "\n",
    "t.format(country = \"서울\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FewShotPromptTemplate\n",
    "# Fewshot은 모델에게 예제들을 준다는 뜻과 같다. -> 더 나은 대답을 할 수 있도록 예제를 준다.\n",
    "\n",
    "# FewShotPromptTemplate\n",
    "# PromptTemplate\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "    Human: {question}\n",
    "    AI: {answer}\n",
    "\"\"\"\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "# prompt.format(country= \"korea\")\n",
    "chain = prompt | chat\n",
    "chain.invoke({\n",
    "    \"country\": \"한국\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FewShotChatMessagePromptTemplate\n",
    "# ChatPromptTemplate\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{country}에 대해 설명해줘\"),\n",
    "    (\"ai\", \"{answer}\")\n",
    "])\n",
    "\n",
    "prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 지리 전문가야, 짧게 말해줘\"),\n",
    "    prompt,\n",
    "    (\"human\", \"{country}에 대해 설명해줘\")\n",
    "])\n",
    "\n",
    "chain = final_prompt | chat\n",
    "chain.invoke({\"country\": \"Korea\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LengthBasedExampleSelector\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector        \n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "        \n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "        \n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "        return [choice(self.examples)]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "\n",
    "# example_selector = LengthBasedExampleSelector(\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples=examples\n",
    "    # example_prompt=example_prompt,\n",
    "    # max_length=180\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt.format(country= \"korea\")\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"country\": \"Korea\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialization and Composition\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "prompt1 = load_prompt(\"./prompt.json\")\n",
    "prompt2 = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "a = prompt1.format(country=\"korea\")\n",
    "b = prompt2.format(country=\"korea\")\n",
    "print(a, \"\\n\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PipelinePromptTemplate: prompt가 많을 때 유용\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    당신은 역할놀이 보조자입니다. 그리고 당신은 {character} 역을 맡아야 합니다.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    이건 어떻게 말해야 하는지 예시야:\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "    시작!\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "    {intro}\n",
    "    \n",
    "    {example}\n",
    "    \n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start)\n",
    "]\n",
    "full_prompt = PipelinePromptTemplate(final_prompt=final, pipeline_prompts=prompts)\n",
    "\n",
    "# 확인\n",
    "# full_prompt.format(\n",
    "#     character = \"해적\",\n",
    "#     example_question = \"어디에 있어?\",\n",
    "#     example_answer = \"크하하하! 그건 비밀이다!\",\n",
    "#     question = \"너가 가장 좋아하는 음식은 뭐야?\"\n",
    "# )\n",
    "\n",
    "chain = full_prompt | chat_model\n",
    "chain.invoke({\n",
    "    \"character\": \"해적\",\n",
    "    \"example_question\": \"어디에 있어?\",\n",
    "    \"example_answer\": \"크하하하! 그건 비밀이다!\",\n",
    "    \"question\": \"너가 가장 좋아하는 음식은 뭐야?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caching: 같은 질문을 계속 받으면 답변을 재사용해 비용을 절약할 수 있게 해준다.\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "set_debug(True)\n",
    "# 무슨 일을 하고 있는지에 대한 로그를 보여준다.\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "chat_model.predict(\"토마토 스파게티는 어떻게 만들어?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model.predict(\"토마토 스파게티는 어떻게 만들어?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caching: 같은 질문을 계속 받으면 답변을 재사용해 비용을 절약할 수 있게 해준다.\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "chat_model.predict(\"토마토 스파게티는 어떻게 만들어?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지는지 궁금하시군요. 소주는 주로 쌀, 보리, 감자 등의 원료를 발효시켜 만들어집니다. 먼저 선택한 곡물을 발효시켜 알코올을 얻은 후, 증류 과정을 거쳐 순수한 주정을 얻습니다. 이후 숙성을 거쳐 부드럽고 깔끔한 맛을 내는 과정을 거칩니다. 소주의 맛과 향은 발효 및 숙성 과정에서 나오는 효모와 박테리아의 영향을 받기 때문에 다양한 종류의 소주가 만들어집니다. 각 지역마다 특색 있는 소주가 만들어지는데, 한국의 전통주인 소주는 다양한 발효 및 숙성 기술을 통해 만들어지며 맛과 향이 다양하게 변화합니다. \n",
      " 네, 김쫀떡은 한국의 전통적인 떡으로, 쫄깃한 떡에 김가루를 묻혀 만든 떡입니다. 해물이나 쇠고기 등 다양한 재료를 넣어 만들기도 하며, 소금이나 간장 등으로 맛을 낼 수 있습니다. 맛있는 한국 전통 간식 중 하나입니다. \n",
      "\n",
      "Tokens Used: 458\n",
      "\tPrompt Tokens: 38\n",
      "\tCompletion Tokens: 420\n",
      "Successful Requests: 2\n",
      "Total Cost (USD): $0.000897\n"
     ]
    }
   ],
   "source": [
    "# Serialization\n",
    "# 모델을 어떻게 저장하고 불러오는지\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "with get_openai_callback() as usage:\n",
    "    a = chat.predict(\"소주는 어떻게 만들어\")\n",
    "    b = chat.predict(\"김쫀떡 알아?\")\n",
    "    print(a, \"\\n\", b, \"\\n\")\n",
    "    print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jangtaehun/Desktop/study/nomad/Fullstack-GPT/env/lib/python3.11/site-packages/langchain/llms/openai.py:216: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/jangtaehun/Desktop/study/nomad/Fullstack-GPT/env/lib/python3.11/site-packages/langchain/llms/openai.py:811: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OpenAIChat(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-16k', model_kwargs={'temperature': 0.1, 'max_tokens': 450, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "# chat = OpenAI(temperature=0.1, max_tokens=450, model=\"gpt-3.5-turbo-16k\")\n",
    "# chat.save(\"model.json\")\n",
    "\n",
    "chat = load_llm(\"model.json\")\n",
    "chat "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
