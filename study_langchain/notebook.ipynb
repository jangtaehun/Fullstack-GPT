{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1\n",
    ")\n",
    "# temperature: 얼마나 창의적인가 0~1\n",
    "\n",
    "template = PromptTemplate.from_template(\n",
    "    \"{country_a}과(와) {country_b} 사이의 거리는 어떻게 되나요?\"\n",
    ")\n",
    "\n",
    "prompt = template.format(country_a = \"한국\", country_b= \"일본\")\n",
    "\n",
    "\n",
    "chat.predict(prompt)\n",
    "\n",
    "# from langchain.llms import OpenAI\n",
    "# llm = OpenAI(model_name=\"gpt-3.5-turbo-0125\")\n",
    "# a = llm.predict(\"How many planets are there?\")\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography export. And you only reply in {language}.\"),\n",
    "    (\"ai\", \"안녕하세요. 제 이름은 {name}입니다.\"),\n",
    "    (\"human\", \"{country_a}과(와) {country_b} 사이의 거리는 어떻게 되나요?. 그리고 당신의 이름은 무엇인가요?\"),\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    language = \"korean\",\n",
    "    name = \"쫀떡\",\n",
    "    country_a = \"한국\",\n",
    "    country_b = \"일본\",\n",
    ")\n",
    "\n",
    "chat.predict_messages(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "    # strip: text 앞뒤 공백을 잘라준다. -> 텍스트ㅡ이 시작부분이나 끝부분이 공백이라면 제거\n",
    "    \n",
    "p = CommaOutputParser()\n",
    "p.parse(\"Hello,   how,are,you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 list 생성 기계입니다. 질문들은 모두 최대 {max_items}개, comma로 구분해주세요. 그리고 영어로 답할 경우 소문자로만 답해주세요. list가 아닌 것으로 답을 하지마세요\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "    ])\n",
    "\n",
    "prompt = template.format_messages(max_items=  \"10\", question = \"What are the colors?\")\n",
    "result = chat.predict_messages(prompt)\n",
    "p = CommaOutputParser()\n",
    "p.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "    # strip: text 앞뒤 공백을 잘라준다. -> 텍스트ㅡ이 시작부분이나 끝부분이 공백이라면 제거\n",
    "    \n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 list 생성 기계입니다. 질문들은 모두 최대 {max_items}개, comma로 구분해주세요. 그리고 영어로 답할 경우 소문자로만 답해주세요. list가 아닌 것으로 답을 하지마세요\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "    ])\n",
    "\n",
    "prompt = template.format_messages(max_items=  \"10\", question = \"What are the colors?\")\n",
    "result = chat.predict_messages(prompt)\n",
    "p = CommaOutputParser()\n",
    "p.parse(result.content)\n",
    "\n",
    "# chat model, outputParser, template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain: 모든 요소를 합쳐주는 역할을 한다.\n",
    "# 합쳐진 요소들은 하나의 chain으로 실행된다. \n",
    "\n",
    "from langchain.chat_models import ChatOpenAI, ChatAnthropic\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "# ChatModel\n",
    "# chat = ChatAnthropic(temperature=0.1)\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "\n",
    "# OutputParser\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "    # strip: text 앞뒤 공백을 잘라준다. -> 텍스트ㅡ이 시작부분이나 끝부분이 공백이라면 제거\n",
    "    \n",
    "    \n",
    "# Prompt\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 list 생성 기계입니다. 질문들은 모두 최대 {max_items}개, comma로 구분해주세요. 그리고 영어로 답할 경우 소문자로만 답해주세요. list가 아닌 것으로 답을 하지마세요\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "    ])\n",
    "\n",
    "\n",
    "chain_one = template | chat | CommaOutputParser()\n",
    "# chain_two = template_2 | chat | outputparser\n",
    "# all = chain_one | chain_two | outputparser \n",
    "\n",
    "chain_one.invoke({\n",
    "    \"max_items\": 5,\n",
    "    \"question\": \"What are the pokemons?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "# streaming: model의 응답이 생성되는 걸 볼 수 있게 해준다.\n",
    "\n",
    "chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 세계적인 요리사입니다. 당신은 쉬운 제료로 쉽게 만들 수 있는 음식 제조법을 알려줘야 합니다.\"),\n",
    "    (\"human\", \"나는 {cuisine}을 만들고 싶습니다.\"),\n",
    "])\n",
    "\n",
    "chef_chain = chef_prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a vegetarian chef specialized on making traditional recipes vegetarian. You find alternative ingredients and explain their preparation. You don't radically modify the recipe. If there is no alternative for a food just say you don't know how to recipe it.\"),\n",
    "    (\"human\", \"{recipe}\"),\n",
    "])\n",
    "\n",
    "veg_chef_chain = veg_chef_prompt | chat_model\n",
    "\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chef_chain # veg_chain의 입력값 recipe를 chef_chain으로부터 뱓는다.\n",
    "final_chain.invoke({\n",
    "    \"cuisine\":\"카레\",\n",
    "})\n",
    "# chef_chain으로부터 얻은 결과가 recipe라는 key에 저장된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모듈\n",
    "LangChain은 다음과 같은 주요 모듈을 위한 확장 가능한 표준 인터페이스 및 외부 통합 기능을 제공합니다:\n",
    "\n",
    "- Model I/O\n",
    "언어 모델과의 인터페이스\n",
    "\n",
    "- Retrieval\n",
    "애플리케이션별 데이터를 사용한 인터페이스\n",
    "\n",
    "- Agents\n",
    "높은 수준의 지침이 주어지면 체인이 어떤 도구를 사용할지 선택하도록 합니다\n",
    "\n",
    "- Chains\n",
    "일반적인 빌딩 블록 구성\n",
    "\n",
    "- Memory\n",
    "체인 실행 간 지속적인 응용 프로그램 상태\n",
    "\n",
    "- Callbacks\n",
    "모든 체인의 중간 단계 기록 및 스트리밍\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:\n",
      "        I know this:\n",
      "        Capital: Ankara\n",
      "        Language: Turkish\n",
      "        Food: Kebab and Baklava\n",
      "        Currency: Turkish Lira"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='AI:\\n        I know this:\\n        Capital: Ankara\\n        Language: Turkish\\n        Food: Kebab and Baklava\\n        Currency: Turkish Lira')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# t = PromptTemplate(\n",
    "#     template=\"What is the capital of {country}\",\n",
    "#     input_variables = [\"country\"],\n",
    "# )\n",
    "# t = PromptTemplate.from_template(\"What is the capital of {country}\")\n",
    "# t.format(country=\"korea\")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI:{answer}\")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke({\"country\": \"Turkey\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국에 대해 알려드리겠습니다:\n",
      "수도: 서울\n",
      "언어: 한국어\n",
      "음식: 김치, 불고기\n",
      "통화: 대한민국 원"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='한국에 대해 알려드리겠습니다:\\n수도: 서울\\n언어: 한국어\\n음식: 김치, 불고기\\n통화: 대한민국 원')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# example_template = \"\"\"\n",
    "#     Human: {question}\n",
    "#     Ai: {answer}\n",
    "# \"\"\"\n",
    "# example_prompt = PromptTemplate.from_template(example_template)\n",
    "# example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAi: {answer}\")\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{country}에 대해 알려줘\"),\n",
    "    (\"ai\", \"{answer}\")\n",
    "])\n",
    "\n",
    "# prompt = FewShotPromptTemplate(\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    # suffix=\"Human: What do you know about {country}?\",\n",
    "    # input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 지리 전문가야\"),\n",
    "    example_prompt,\n",
    "    (\"human\", \"{country}에 대해 알려줘\" ),\n",
    "])\n",
    "\n",
    "chain = final_prompt | chat_model\n",
    "chain.invoke({\n",
    "    \"country\": \"Korea\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국에 대해 알려드리겠습니다:\n",
      "수도: 서울\n",
      "언어: 한국어\n",
      "음식: 김치, 불고기\n",
      "통화: 대한민국 원"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='한국에 대해 알려드리겠습니다:\\n수도: 서울\\n언어: 한국어\\n음식: 김치, 불고기\\n통화: 대한민국 원')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "# from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# example_template = \"\"\"\n",
    "#     Human: {question}\n",
    "#     Ai: {answer}\n",
    "# \"\"\"\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{country}에 대해 알려줘\"),\n",
    "    (\"ai\", \"{answer}\")\n",
    "])\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 지리 전문가야\"),\n",
    "    example_prompt,\n",
    "    (\"human\", \"{c}에 대해 알려줘\" ),\n",
    "])\n",
    "\n",
    "chain = final_prompt | chat_model\n",
    "chain.invoke({\n",
    "    \"c\": \"Korea\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# 예제 선택기 \n",
    "class RandomExampleSelect(BaseExampleSelector):\n",
    "    \n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "    \n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "    \n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "        return [choice(self.examples)]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAi: {answer}\")\n",
    "\n",
    "example_selector = RandomExampleSelect(\n",
    "    examples = examples,\n",
    ")\n",
    "\n",
    "example_prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix = \"Human: {country}에 대해 알려줘\",\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Korea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialization\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "# prompt = load_prompt(\"./prompt.json\")\n",
    "prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "prompt.format(country= \"Korea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    당신은 역할놀이 보조자입니다. 그리고 당신은 {character} 역을 맡아야 합니다.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    이건 어떻게 말해야 하는지 예시야:\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "    시작!\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "    {intro}\n",
    "    \n",
    "    {example}\n",
    "    \n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start)\n",
    "]\n",
    "full_prompt = PipelinePromptTemplate(final_prompt=final, pipeline_prompts=prompts)\n",
    "\n",
    "# 확인\n",
    "# full_prompt.format(\n",
    "#     character = \"해적\",\n",
    "#     example_question = \"어디에 있어?\",\n",
    "#     example_answer = \"크하하하! 그건 비밀이다!\",\n",
    "#     question = \"너가 가장 좋아하는 음식은 뭐야?\"\n",
    "# )\n",
    "\n",
    "chain = full_prompt | chat_model\n",
    "chain.invoke({\n",
    "    \"character\": \"해적\",\n",
    "    \"example_question\": \"어디에 있어?\",\n",
    "    \"example_answer\": \"크하하하! 그건 비밀이다!\",\n",
    "    \"question\": \"너가 가장 좋아하는 음식은 뭐야?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caching: 같은 질문을 계속 받으면 답변을 재사용해 비용을 절약할 수 있게 해준다.\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "# set_llm_cache(InMemoryCache())\n",
    "set_debug(True)\n",
    "# 무슨 일을 하고 있는지에 대한 로그를 보여준다.\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "chat_model.predict(\"토마토 스파게티는 어떻게 만들어?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model.predict(\"토마토 스파게티는 어떻게 만들어?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지출하는 비용을 아는 방법\n",
    "# 모델을 어떻게 저장하고 불러오는지에 대한 방법\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "with get_openai_callback() as test:\n",
    "    a = chat_model.predict(\"소주 만드는 방법 알려줘\")\n",
    "    b = chat_model.predict(\"토마토 스파게티 만드는 법 알려줘\")\n",
    "    print(a, b, \"\\n\")\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms.openai import OpenAI\n",
    "\n",
    "chat = OpenAI(temperature=0.1, max_tokens=450, model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "chat.save(\"model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "chat = load_llm(\"model.json\")\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain에는 여러 종류의 메모리가 있다. / 각자 저장 방식도 다르다\n",
    "# 챗봇에 메모리를 추가하지 않으면 기억할 수 없다.\n",
    "# 메모리 종류, 차이점 -> 메모리 탑재 방법\n",
    "\n",
    "\n",
    "# 1. Conversation Buffer = 단순, 이전 대화 내용 전체를 저장 / 대화가 길어지면 메모리도 커져 비효율적이다.\n",
    "# text completion 할 때 유용, 예측할 때, 텍스트를 자동완성하고 싶을 때 \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# memory = ConversationBufferMemory()\n",
    "memory = ConversationBufferMemory(return_messages=True) # 챗모델 사용\n",
    "\n",
    "memory.save_context({\"input\":\"Hi\"}, {\"output\": \"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. conversation Buffer Window Memory\n",
    "# 대화 기록 최소화, 가장 오래된 메시지는 버린다. -> 모든 대화 내용을 저장하지 않아도 된다. / 최근 대화에만 집중한다.\n",
    "# 메모리의 사이즈가 게속 늘어나지 않지만, 챗봇이 예전 대화를 기억하지 못 한다.\n",
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k = 4 # 버퍼 윈도우의 사이즈 = 몇 개의 메세지를 저장할 것인가\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(1, 1)\n",
    "add_message(2, 2)\n",
    "add_message(3, 3)\n",
    "add_message(4, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Conversation Summary Memory\n",
    "# message를 그대로 저장하지 않고 요약을 자체적으로 해준다. -> 긴 conversation에서 효과적이다.\n",
    "# 초반에는 Conversation Summary Memory는 이전보다 더 많은 토큰과 저장공간을 차지하지만 이후에 효과적\n",
    "\n",
    "# 시간이 지남에 따라 대화의 요약 -> 시간이 지남에 따라 대화의 정보를 압축하는 데 유용\n",
    "# 대화가 진행되는 대로 요약하고 현재 요약 내용을 메모리에 저장 \n",
    "\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "    \n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"안녕 나는 장태훈이야, 나는 한국에서 살아\", \"와 이거 정말 멋진데!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"한국은 정말 멋져!!\", \"나도 갈 수 있었으면 좋겠다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human introduces themselves as Jang Taehoon from Korea. The AI responds enthusiastically to the introduction, expressing a desire to visit Korea as well.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Conversation Summary Buffer Memory\n",
    "# 두 가지 아이디어 결합 -> 메모리에 최근 상호 작용의 버퍼 저장 + 오래된 상호 작용을 요약으로 컴파일 -> 가장 최근의 상호작용을 계속 추적\n",
    "# 메모리에 보내온 메시지의 수를 저장, \n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=150, return_messages=True)\n",
    "# max_token_limit=10: 가능한 메시지 토큰 수의 최대값\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "    \n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"안녕 나는 장태훈이야, 나는 한국에서 살아\", \"와 이거 정말 멋진데!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human introduces themselves as Jang Taehoon from Korea. The AI responds enthusiastically to the introduction.')]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"한국은 정말 멋져!!\", \"나도 갈 수 있었으면 좋겠다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human introduces themselves as Jang Taehoon from Korea and expresses admiration for the country. The AI responds enthusiastically to the introduction and expresses a desire to visit Korea.')]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"한국이랑 아르헨나까지의 거리가 얼마나 되는지 알려 줄 수 있어?\", \"미안해!! 나도 잘 모르겠어 근데 엄청 멀꺼야 지구 반대퍈에 있거든!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human introduces themselves as Jang Taehoon from Korea and expresses admiration for the country. The AI responds enthusiastically to the introduction and expresses a desire to visit Korea, but admits not knowing the distance between Korea and Argentina, guessing it must be very far since they are on opposite sides of the Earth.')]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"곧 대한민국에서는 총선이야. 총선은 국회의원을 뽑는 선거인데 이번엔 누가 뽑힐지 잘 모르겠어 너는 혹시 알아? 알려줄 수 있어?\", \"미안해!! 나도 잘 모르겠어 근데 일 잘 하는 사람이 뽑힐 거야!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human introduces themselves as Jang Taehoon from Korea and expresses admiration for the country. The AI responds enthusiastically to the introduction and expresses a desire to visit Korea, but admits not knowing the distance between Korea and Argentina, guessing it must be very far since they are on opposite sides of the Earth. Jang Taehoon mentions the upcoming general elections in Korea and asks if the AI knows who might be elected. The AI apologizes for not knowing but believes that competent individuals will be chosen.')]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Conversation KG(knowledfe graph) Memory\n",
    "# 대화 중의 엔티티의 knowledge graph를 만든다. -> 가장 중요한 것들만 뽑아내는 요약본\n",
    "\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm, return_messages=True)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "    \n",
    "\n",
    "add_message(\"안녕 나는 장태훈이야, 나는 한국에서 살아\", \"와 이거 정말 멋진데!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human introduces themselves as Jang Taehoon from Korea. The AI responds enthusiastically to the introduction.')]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"장태훈이 누구인가요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"장태훈은 고양이를 좋아해 그 중 쫀떡이를 제일 좋아해\", \"와 이거 정말 멋진데!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human introduces themselves as Jang Taehoon from Korea, mentioning their love for cats and specifically Jjondeok. The AI responds enthusiastically to the introduction, expressing admiration for their love of cats.')]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"장태훈이 좋아하는 건 뭐야?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    너는 사람과 대화하며 도움을 주는 AI야\n",
      "    \n",
      "    Human:내 이름은 장태훈이야\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'안녕하세요, 장태훈님! 만나서 반가워요. 무엇을 도와드릴까요?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM chain\n",
    "# off-the-shelf chain = 일반적인 목적을 가진 chain\n",
    "# 스스로 무언가를 만들어 볼 때는 직접 커스텀해서 만든 chain을 활용하는 것이 좋다.\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# 메모리는 template 안에 대화 기록을 저장하는 공간을 찾을 거고 자동으로 그 메모리의 기록을 chat_history에 넣는다.\n",
    "# memory_key=\"chat_history\" => 메모리를 위한 공간을 만들어 준다.\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=120, memory_key=\"chat_history\")\n",
    "\n",
    "template = \"\"\"\n",
    "    너는 사람과 대화하며 도움을 주는 AI야\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.predict(question=\"내 이름은 장태훈이야\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    너는 사람과 대화하며 도움을 주는 AI야\n",
      "    Human: 내 이름은 장태훈이야\n",
      "AI: 안녕하세요, 장태훈님! 만나서 반가워요. 무엇을 도와드릴까요?\n",
      "    Human:나는 수원에서 살아\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'수원은 경기도에 위치한 아름다운 도시죠. 수원에서 좋아하는 장소나 음식이 있나요? 무엇이든 물어보세요!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"나는 수원에서 살아\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    너는 사람과 대화하며 도움을 주는 AI야\n",
      "    System: The human introduces themselves as 장태훈.\n",
      "AI: 안녕하세요, 장태훈님! 만나서 반가워요. 무엇을 도와드릴까요?\n",
      "Human: 나는 수원에서 살아\n",
      "AI: 수원은 경기도에 위치한 아름다운 도시죠. 수원에서 좋아하는 장소나 음식이 있나요? 무엇이든 물어보세요!\n",
      "    Human:내 이름이 뭐야\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AI: 죄송해요, 장태훈님. 제가 이름을 가지고 있지는 않아요. 하지만 저는 여러분의 질문에 답변해 드릴 수 있어요. 무엇이든 물어보세요!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"내 이름이 뭐야\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': 'System: The human introduces themselves as 장태훈. The AI greets them and asks how it can help. The human mentions they live in Suwon. The AI comments on Suwon being a beautiful city in Gyeonggi-do and asks if there are any favorite places or foods in Suwon.\\nHuman: 내 이름이 뭐야\\nAI: AI: 죄송해요, 장태훈님. 제가 이름을 가지고 있지는 않아요. 하지만 저는 여러분의 질문에 답변해 드릴 수 있어요. 무엇이든 물어보세요!'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 너는 사람과 대화하며 도움을 주는 AI야\n",
      "Human: 내 이름은 장태훈이야\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'안녕, 장태훈! 무엇을 도와드릴까요?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 대화 기반의 채팅으로 사용\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=120, memory_key=\"chat_history\", return_messages=True)\n",
    "# 문자열로 바꾸지 말고 실제 메세지로 바꿔달라는 것을 의미\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 사람과 대화하며 도움을 주는 AI야\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "# ConversationBufferMemory로부터 요약본을 받아올 때 시스템 메시지도 추가된다.\n",
    "# MessagesPlaceholder(variable_name=\"chat_history\"): 메세지가 얼마나 많고 누구에게로부터 왔는지 모른다 -> memory class로 대체된다.\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.predict(question=\"내 이름은 장태훈이야\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 너는 사람과 대화하며 도움을 주는 AI야\n",
      "Human: 내 이름은 장태훈이야\n",
      "AI: 안녕, 장태훈! 무엇을 도와드릴까요?\n",
      "Human: 나는 수원에서 살아\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'수원은 정말 멋진 도시야! 수원에서 좋아하는 장소나 취미 활동이 있나요? 무엇인가 추천해 드릴까요?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"나는 수원에서 살아\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 너는 사람과 대화하며 도움을 주는 AI야\n",
      "System: The human introduces themselves as 장태훈.\n",
      "AI: 안녕, 장태훈! 무엇을 도와드릴까요?\n",
      "Human: 나는 수원에서 살아\n",
      "AI: 수원은 정말 멋진 도시야! 수원에서 좋아하는 장소나 취미 활동이 있나요? 무엇인가 추천해 드릴까요?\n",
      "Human: 내 이름이 뭔지 알아?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'네, 당신의 이름은 장태훈이라고 말씀하셨죠. 어떻게 도와드릴까요, 장태훈님?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"내 이름이 뭔지 알아?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL Based Memory\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=120, memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"너는 사람과 대화하며 도움을 주는 AI야\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "# ConversationBufferMemory로부터 요약본을 받아올 때 시스템 메시지도 추가된다.\n",
    "# MessagesPlaceholder(variable_name=\"chat_history\"): 메세지가 얼마나 많고 누구에게로부터 왔는지 모른다 -> memory class로 대체된다.\n",
    "\n",
    "def load_memory(input):\n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\n",
    "        # \"chat_history\": load_memory()\n",
    "        \"question\": question\n",
    "    })\n",
    "    memory.save_context({\"input\": question}, {\"output\": result.content})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='안녕, 장태훈! 무엇을 도와드릴까요?'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"내 이름은 장태훈이야\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='네, 당신의 이름은 장태훈이라고 말씀하셨죠. 어떻게 도와드릴까요?'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"내 이름이 뭔지 알아?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
