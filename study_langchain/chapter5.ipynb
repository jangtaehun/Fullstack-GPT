{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### memory\n",
    "> - 메모리 여러 개 -> 각자 저장 방식이 다르며, 각자만의 장단점이 있다.\n",
    "> - 챗복에 메모리를 추가하지 않으면 챗복은 아무것도 기억할 수 없다. -> 이전 질문에 이어지는 질문을 기억하지 못 한다.\n",
    "> - 메모리가 있어야 어떤 사람과 얘기하고 있다는 느낌을 들게 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation Buffer 메모리\n",
    "> - 이전 대화 내용 전체를 저장하는 메모리\n",
    "> - 대화가 길어질 수록 메모리도 계속 커져서 비효율적이다. / 이해하기 가장 쉬운 메모리\n",
    "> - 메모리 종류와 무관하게 API는 다 똑같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi'), AIMessage(content='hello')]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversation Buffer 메모리\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"hello\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi'),\n",
       "  AIMessage(content='hello'),\n",
       "  HumanMessage(content='Hi'),\n",
       "  AIMessage(content='hello')]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"hello\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation Buffer Window 메모리\n",
    "> - 대화의 특정 부분, 가장 최근 부분만 저장하는 메모리 + 메모리 크기를 지정할 수 있다.\n",
    "> - 챗봇이 전체 대화가 아닌 최근 대화에만 집중한다. / 모든 대화를 저장하지 않는다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='1'),\n",
       "  AIMessage(content='1'),\n",
       "  HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4')]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversation Buffer Window 메모리\n",
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k = 4\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"outputs\":output})\n",
    "\n",
    "add_message(1, 1)\n",
    "add_message(2, 2)\n",
    "add_message(3, 3)\n",
    "add_message(4, 4)\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5'),\n",
       "  HumanMessage(content='6'),\n",
       "  AIMessage(content='6')]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(5, 5)\n",
    "add_message(6, 6)\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation Summay 메모리\n",
    "> - llm을 사용, ChatopenAI로부터 chat_model llm을 import -> 비용이 든다.\n",
    "> - 그대로 저장하는 것이 아닌 요약을 자체적으로 한다.\n",
    "> - 초반에는 많은 토큰과 저장공간을 차지하지만 후반에는 효과적이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"outputs\":output})\n",
    "    \n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"안녕 나는 쫀떡이야. 나는 대한민국에서 살아\", \"와 엄청 멋진 이름이야!!\")\n",
    "add_message(\"쫀떡이는 고양이야\", \"와 귀엽다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human introduces themselves as 쫀떡 and mentions they live in South Korea. The AI compliments their name as cool and reacts with excitement to the mention of a cat named 밥생밥사야 who loves food and snacks. The AI finds it cute.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"쫀떡이는 밥을 너무 좋아해서 별명이 밥생밥사야. 간식도 엄청 좋아해 그래서 토실토실해\", \"와 귀엽다\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation Summary Buffer 메모리\n",
    "> - Conversation Summay memory + Conversation Buffer memory\n",
    "> - 메모리에 보내온 메시지의 수를 저장, limit에 다다른 순간 오래된 메시지들을 요약한다.\n",
    "> - 가장 최근의 상호작용을 계속 추적 + 오래된 메시지 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human introduces themselves as 쫀떡 and mentions they live in South Korea. The AI compliments the name and reacts with \"와 귀엽다\" when told that 쫀떡이는 고양이야. 쫀떡이는 밥을 너무 좋아해서 별명이 밥생밥사야. 간식도 엄청 좋아해 그래서 토실토실해. The AI thinks chubby cats are the cutest!')]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm, max_token_limit=150, return_messages=True)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"outputs\":output})\n",
    "    \n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"안녕 나는 쫀떡이야. 나는 대한민국에서 살아\", \"와 엄청 멋진 이름이야!!\")\n",
    "add_message(\"쫀떡이는 고양이야\", \"와 귀엽다\")\n",
    "add_message(\"쫀떡이는 밥을 너무 좋아해서 별명이 밥생밥사야. 간식도 엄청 좋아해 그래서 토실토실해\", \"뚱뚱한 고양이는 제일 귀여워!!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation KG 메모리\n",
    "> - 대화 도중의 엔티티의 knowledge graph를 만든다.\n",
    "> - 가장 중요한 것들만 뽑아내는 요약본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On 쫀떡이: 쫀떡이 는 고양이. 쫀떡이 좋아해서 밥. 쫀떡이 별명이 밥생밥사. 쫀떡이 좋아해 간식. 쫀떡이 토실토실해 .')]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(llm=llm, return_messages=True)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"outputs\":output})\n",
    "\n",
    "\n",
    "add_message(\"안녕 나는 쫀떡이야. 나는 대한민국에서 살아\", \"와 엄청 멋진 이름이야!!\")\n",
    "add_message(\"쫀떡이는 고양이야\", \"와 귀엽다\")\n",
    "add_message(\"쫀떡이는 밥을 너무 좋아해서 별명이 밥생밥사야. 간식도 엄청 좋아해 그래서 토실토실해\", \"뚱뚱한 고양이는 제일 귀여워!!\")\n",
    "memory.load_memory_variables({\"input\": \"쫀떡이가 좋아하는 게 뭐야\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory on LLMChain\n",
    "> - LLM chain은 off-the-shelf chain: 일반적인 목적을 가진 chain\n",
    "> - 빠르게 시작할 수 있다.\n",
    "> - 스스로 무언가를 만들어볼 때 off-the-shelf chain 보다는 직접 커스텀해서 만든 chain을 활용하는 것이 좋다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "    너는 대화를 도와주는 AI야\n",
      "    \n",
      "    \n",
      "    human: 내 이름은 김쫀떡이야\n",
      "    you: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'안녕, 김쫀떡이! 만나서 반가워. 무엇을 도와드릴까?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 메모리 클래스가 메모리를 두 가지 방식으로 출력할 수 있다. -> 문자열 형태, message 형태\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=120, memory_key=\"chat_history\")\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "    너는 대화를 도와주는 AI야\n",
    "    \n",
    "    {chat_history}\n",
    "    human: {question}\n",
    "    you: \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt= PromptTemplate.from_template(template),\n",
    "    verbose=True # 프롬프트 로그들을 확인할 수 있다.\n",
    ")\n",
    "\n",
    "chain.predict(question=\"내 이름은 김쫀떡이야\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "    너는 대화를 도와주는 AI야\n",
      "    \n",
      "    System: The human introduces themselves as Kim Jjeontteogi. The AI responds with a greeting and asks how it can help Kim Jjeontteogi. Kim Jjeontteogi mentions living in Seoul, to which the AI responds positively about the city and offers assistance with any information needed about tourist spots or local culture.\n",
      "Human: 내 이름이 뭐야\n",
      "AI: 당신의 이름은 김 쩐떼오기씨로 소개하셨죠. 저는 여러분이 필요로 하는 정보를 제공해드릴 수 있어요. 무엇을 도와드릴까요?\n",
      "    human: 나는 서울에서 살아\n",
      "    you: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AI: 서울은 정말 멋진 도시에요! 관광지나 현지 문화에 대해 궁금한 점이 있으면 언제든지 물어봐주세요. 어떤 도움이 필요하신가요?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"나는 서울에서 살아\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "    너는 대화를 도와주는 AI야\n",
      "    \n",
      "    System: The human introduces themselves as Kim Jjeontteogi. The AI responds with a greeting and offers assistance. Kim Jjeontteogi asks for their name in Korean, to which the AI responds with their name.\n",
      "Human: 나는 서울에서 살아\n",
      "AI: AI: 서울은 정말 멋진 도시에요! 관광지나 현지 문화에 대해 궁금한 점이 있으면 언제든지 물어봐주세요. 어떤 도움이 필요하신가요?\n",
      "Human: 내 이름이 뭐야\n",
      "AI: 나는 AI야.\n",
      "    human: 내 이름이 뭐야\n",
      "    you: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'네 이름은 김 쩐떼오기야.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"내 이름이 뭐야\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': 'System: Kim Jjeontteogi introduces themselves and the AI responds with a greeting and offers assistance. Kim Jjeontteogi mentions they live in Seoul and the AI expresses admiration for the city, offering help with any questions about tourist spots or local culture.\\nHuman: 내 이름이 뭐야\\nAI: 나는 AI야.\\nHuman: 내 이름이 뭐야\\nAI: 네 이름은 김 쩐떼오기야.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat Based Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 너는 대화를 통해 인간을 도와주는 AI야\n",
      "Human: 내 이름은 김쫀떡\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'안녕하세요, 김쫀떡님! 무엇을 도와드릴까요?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=120, memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"너는 대화를 통해 인간을 도와주는 AI야\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt= prompt,\n",
    "    verbose=True # 프롬프트 로그들을 확인할 수 있다.\n",
    ")\n",
    "\n",
    "chain.predict(question=\"내 이름은 김쫀떡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL Based Memory\n",
    "# 메모리 추가\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=120, memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"너는 대화를 통해 인간을 도와주는 AI야\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def load_memory(input):\n",
    "    # chain에 있는 모든 컴포넌트는 input을 받고 output을 내준다.\n",
    "    print(input)\n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context({\"input\": question}, {\"putput\": result.content})\n",
    "    print(result)\n",
    "\n",
    "# RunnablePassthrough => 프롬프트가 format되기 전에 우리가 함수를 실행시키는 걸 허락해준다.\n",
    "# RunnablePassthrough.assign(chat_history=load_memory)\n",
    "# 실행할 때 가장 먼저 load_memory 함수를 호출 -> 프롬프트가 필요로 하는 chat_history key 내부에 넣는다.(load_memory 함수의 결과값을 chat_history input에 넣는다)\n",
    "\n",
    "# MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "# chat_history가 필요한 프롬프트로 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '내 이름은 쫀떡'}\n",
      "content='안녕, 쫀떡! 무엇을 도와드릴까요?'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"내 이름은 쫀떡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '내 이름이 뭐야'}\n",
      "content='당신의 이름은 쫀떡이에요. 저도 기억하고 있어요! 계속 대화를 이어가거나 다른 질문이 있으면 언제든지 물어봐주세요.'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"내 이름이 뭐야\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
